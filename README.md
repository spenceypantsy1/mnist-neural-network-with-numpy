# MNIST Neural Network with Numpy
This project is an attempt to apply fundamental knowledge of the mathematics and calculus behind neural networks. Inspired by Samson Zhang's video, I built this neural network from scratch using only numpy, focusing on the essential concepts behind neural networks, such as architecture, activation functions, forward and backward propagation, and more.

## Overview
In this project, I:
- Implemented a simple feedforward neural network to classify handwritten digits from the MNIST dataset.
- Gained a deeper understanding of how neural networks work under the hood, including the detailed mathematical foundations.
  
## Key Concepts Covered
- **Neural Network Architecture:** Understanding how layers are structured and connected.
- **Activation Functions:** Derivations and applications of activation functions like Sigmoid and ReLU.
- **Forward Propagation:** Propagation of inputs through the network to make predictions.
- **Backpropagation:** Calculating gradients and updating weights using the gradient descent algorithm.

## Inspiration
This project was inspired by the tutorial video from [Samson Zhang](https://www.youtube.com/@SamsonZhangTheSalmon), who explains the neural network structure and operations from the ground up using only numpy.

## References
Deep Learning Basics (Lecture Notes) by Romain Tavenard
[Samson Zhang's Video Tutorial](https://youtu.be/w8yWXqWQYmU?si=KFBjRs50fVVLMsi2)

## Dependencies
```bash 
pip install numpy matplotlib pandas
```

## How to run
1. Clone the repo
```bash
git clone https://github.com/spenceypantsy1/mnist-neural-network-with-numpy.git
```
2. Install dependencies (see above)
3. Run the script or Jupyter notebook
